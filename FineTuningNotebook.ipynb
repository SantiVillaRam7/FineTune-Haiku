{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/haiku-gpt2/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/var/folders/6l/18s2w2wn2bd93c3r28h5qs_h0000gn/T/ipykernel_17915/2885767323.py:16: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  device = torch.device(\"mps\" if torch.has_mps else \"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': \"Write a haiku about the elk's bugling in the forest.\", 'completion': \"Autumn leaves quiver,\\nElk's call echoes through trees,\\nNature's symphony.\", 'label': False, 'label-suggestion': None, 'label-suggestion-metadata': {'type': None, 'score': None, 'agent': None}, 'external_id': None, 'metadata': '{\"prompt\": \"Write a haiku about the elk\\'s bugling in the forest.\", \"generation_model\": \"mistralai/Mistral-7B-Instruct-v0.2\"}', 'messages': [{'content': \"Write a haiku about the elk's bugling in the forest.\", 'role': 'user'}, {'content': \"Autumn leaves quiver,\\nElk's call echoes through trees,\\nNature's symphony.\", 'role': 'assistant'}]}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the tokenizer and model\n",
    "model_name = \"gpt2-large\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Add padding token to the tokenizer\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "model.resize_token_embeddings(len(tokenizer))  # Adjust the model's embedding size\n",
    "\n",
    "# Check if MPS (Metal Performance Shaders) is available\n",
    "device = torch.device(\"mps\" if torch.has_mps else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Load the haiku dataset\n",
    "dataset = load_dataset(\"davanstrien/haiku_kto\")\n",
    "\n",
    "# Inspect dataset keys\n",
    "print(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the training data into train and validation sets (90% train, 10% validation)\n",
    "train_val_split = dataset['train'].train_test_split(test_size=0.1)\n",
    "train_data = train_val_split['train']\n",
    "val_data = train_val_split['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 82/82 [00:00<00:00, 3359.31 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 2413.02 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Extract haiku texts and tokenize them\n",
    "def extract_and_tokenize_function(batch):\n",
    "    haikus = []\n",
    "    for example in batch['messages']:\n",
    "        haiku = next((message['content'] for message in example if message['role'] == 'assistant'), None)\n",
    "        if haiku:\n",
    "            haikus.append(haiku)\n",
    "    tokenized = tokenizer(haikus, truncation=True, padding='max_length', max_length=50)\n",
    "    input_ids = torch.tensor(tokenized['input_ids'])\n",
    "    attention_mask = torch.tensor(tokenized['attention_mask'])\n",
    "    labels = input_ids.clone()\n",
    "    labels[labels == tokenizer.pad_token_id] = -100\n",
    "    return {'input_ids': input_ids, 'attention_mask': attention_mask, 'labels': labels}\n",
    "\n",
    "# Apply tokenization function with batching\n",
    "train_data = train_data.map(extract_and_tokenize_function, batched=True, remove_columns=train_data.column_names)\n",
    "val_data = val_data.map(extract_and_tokenize_function, batched=True, remove_columns=val_data.column_names)\n",
    "\n",
    "train_data.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "val_data.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/haiku-gpt2/lib/python3.9/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",  # Save strategy set to \"epoch\"\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/205 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|â–         | 10/205 [00:12<03:05,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.9015, 'grad_norm': 15.342203140258789, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|â–‰         | 20/205 [00:23<03:43,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.855, 'grad_norm': 15.591681480407715, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|â–ˆâ–        | 30/205 [00:32<02:39,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.4229, 'grad_norm': 16.52741050720215, 'learning_rate': 3e-06, 'epoch': 0.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|â–ˆâ–‰        | 40/205 [01:28<08:02,  2.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.1647, 'grad_norm': 19.260408401489258, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 20%|â–ˆâ–ˆ        | 41/205 [01:45<17:21,  6.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.7416024208068848, 'eval_runtime': 2.2725, 'eval_samples_per_second': 4.4, 'eval_steps_per_second': 2.2, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|â–ˆâ–ˆâ–       | 50/205 [04:37<45:44, 17.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.7262, 'grad_norm': 13.97623062133789, 'learning_rate': 5e-06, 'epoch': 1.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|â–ˆâ–ˆâ–‰       | 60/205 [05:37<08:55,  3.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.4244, 'grad_norm': 15.864128112792969, 'learning_rate': 6e-06, 'epoch': 1.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|â–ˆâ–ˆâ–ˆâ–      | 70/205 [05:49<02:28,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.3003, 'grad_norm': 14.642148971557617, 'learning_rate': 7.000000000000001e-06, 'epoch': 1.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|â–ˆâ–ˆâ–ˆâ–‰      | 80/205 [06:05<04:25,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.973, 'grad_norm': 15.36148452758789, 'learning_rate': 8.000000000000001e-06, 'epoch': 1.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 82/205 [06:08<03:42,  1.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.918102741241455, 'eval_runtime': 0.3873, 'eval_samples_per_second': 25.822, 'eval_steps_per_second': 12.911, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 90/205 [07:14<04:49,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4999, 'grad_norm': 11.069402694702148, 'learning_rate': 9e-06, 'epoch': 2.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 100/205 [07:24<02:21,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4913, 'grad_norm': 12.502325057983398, 'learning_rate': 1e-05, 'epoch': 2.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 110/205 [07:35<01:21,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4232, 'grad_norm': 22.84785270690918, 'learning_rate': 1.1000000000000001e-05, 'epoch': 2.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 120/205 [07:49<03:37,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2908, 'grad_norm': 12.233875274658203, 'learning_rate': 1.2e-05, 'epoch': 2.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 123/205 [07:54<02:36,  1.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.7125401496887207, 'eval_runtime': 0.3606, 'eval_samples_per_second': 27.731, 'eval_steps_per_second': 13.865, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 130/205 [08:45<03:32,  2.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7921, 'grad_norm': 14.437853813171387, 'learning_rate': 1.3000000000000001e-05, 'epoch': 3.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 140/205 [09:02<02:02,  1.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5513, 'grad_norm': 12.577813148498535, 'learning_rate': 1.4000000000000001e-05, 'epoch': 3.41}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 150/205 [09:22<02:53,  3.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7324, 'grad_norm': 18.391651153564453, 'learning_rate': 1.5e-05, 'epoch': 3.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 160/205 [09:34<00:46,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4684, 'grad_norm': 14.281442642211914, 'learning_rate': 1.6000000000000003e-05, 'epoch': 3.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 164/205 [09:37<00:35,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.8714218139648438, 'eval_runtime': 0.3215, 'eval_samples_per_second': 31.108, 'eval_steps_per_second': 15.554, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 170/205 [10:51<02:51,  4.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1427, 'grad_norm': 13.994254112243652, 'learning_rate': 1.7000000000000003e-05, 'epoch': 4.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 180/205 [11:03<00:28,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.949, 'grad_norm': 14.221123695373535, 'learning_rate': 1.8e-05, 'epoch': 4.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 190/205 [11:12<00:13,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9869, 'grad_norm': 13.56718635559082, 'learning_rate': 1.9e-05, 'epoch': 4.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 200/205 [11:24<00:05,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0187, 'grad_norm': 15.463618278503418, 'learning_rate': 2e-05, 'epoch': 4.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 205/205 [11:48<00:00,  3.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.0735347270965576, 'eval_runtime': 0.354, 'eval_samples_per_second': 28.247, 'eval_steps_per_second': 14.124, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 205/205 [12:28<00:00,  3.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 748.1234, 'train_samples_per_second': 0.548, 'train_steps_per_second': 0.274, 'train_loss': 2.5649413271648127, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=205, training_loss=2.5649413271648127, metrics={'train_runtime': 748.1234, 'train_samples_per_second': 0.548, 'train_steps_per_second': 0.274, 'total_flos': 87132019200000.0, 'train_loss': 2.5649413271648127, 'epoch': 5.0})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fine-tune the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./fine-tuned-haiku-model/tokenizer_config.json',\n",
       " './fine-tuned-haiku-model/special_tokens_map.json',\n",
       " './fine-tuned-haiku-model/vocab.json',\n",
       " './fine-tuned-haiku-model/merges.txt',\n",
       " './fine-tuned-haiku-model/added_tokens.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the fine-tuned model\n",
    "model.save_pretrained(\"./fine-tuned-haiku-model\")\n",
    "tokenizer.save_pretrained(\"./fine-tuned-haiku-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate haiku\n",
    "def generate_haiku(prompt, model, tokenizer, max_length=30):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    input_ids = inputs['input_ids']\n",
    "    attention_mask = inputs['attention_mask']\n",
    "    outputs = model.generate(\n",
    "        input_ids=input_ids, \n",
    "        attention_mask=attention_mask, \n",
    "        max_length=max_length, \n",
    "        num_return_sequences=1, \n",
    "        no_repeat_ngram_size=2, \n",
    "        early_stopping= True\n",
    "    )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/haiku-gpt2/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:563: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Haiku:\n",
      "The oceans breeze,\n",
      "Silent whispers of the sea,\n",
      "\n",
      "\n",
      "Nature's symphony, timeless.\n",
      "\n",
      "...\n",
      "\n",
      ".\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the fine-tuned model\n",
    "prompt = \"The oceans breeze\"\n",
    "haiku = generate_haiku(prompt, model, tokenizer)\n",
    "print(f\"Generated Haiku:\\n{haiku}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fineTune",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
