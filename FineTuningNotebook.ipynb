{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6l/18s2w2wn2bd93c3r28h5qs_h0000gn/T/ipykernel_8675/2885767323.py:16: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  device = torch.device(\"mps\" if torch.has_mps else \"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': \"Write a haiku about the elk's bugling in the forest.\", 'completion': \"Autumn leaves quiver,\\nElk's call echoes through trees,\\nNature's symphony.\", 'label': False, 'label-suggestion': None, 'label-suggestion-metadata': {'type': None, 'score': None, 'agent': None}, 'external_id': None, 'metadata': '{\"prompt\": \"Write a haiku about the elk\\'s bugling in the forest.\", \"generation_model\": \"mistralai/Mistral-7B-Instruct-v0.2\"}', 'messages': [{'content': \"Write a haiku about the elk's bugling in the forest.\", 'role': 'user'}, {'content': \"Autumn leaves quiver,\\nElk's call echoes through trees,\\nNature's symphony.\", 'role': 'assistant'}]}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the tokenizer and model\n",
    "model_name = \"gpt2-large\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Add padding token to the tokenizer\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "model.resize_token_embeddings(len(tokenizer))  # Adjust the model's embedding size\n",
    "\n",
    "# Check if MPS (Metal Performance Shaders) is available\n",
    "device = torch.device(\"mps\" if torch.has_mps else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Load the haiku dataset\n",
    "dataset = load_dataset(\"davanstrien/haiku_kto\")\n",
    "\n",
    "# Inspect dataset keys\n",
    "print(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the training data into train and validation sets (90% train, 10% validation)\n",
    "train_val_split = dataset['train'].train_test_split(test_size=0.1)\n",
    "train_data = train_val_split['train']\n",
    "val_data = train_val_split['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract haiku texts and tokenize them\n",
    "def extract_and_tokenize_function(batch):\n",
    "    haikus = []\n",
    "    for example in batch['messages']:\n",
    "        haiku = next((message['content'] for message in example if message['role'] == 'assistant'), None)\n",
    "        if haiku:\n",
    "            haikus.append(haiku)\n",
    "    tokenized = tokenizer(haikus, truncation=True, padding='max_length', max_length=50)\n",
    "    input_ids = torch.tensor(tokenized['input_ids'])\n",
    "    attention_mask = torch.tensor(tokenized['attention_mask'])\n",
    "    labels = input_ids.clone()\n",
    "    labels[labels == tokenizer.pad_token_id] = -100\n",
    "    return {'input_ids': input_ids, 'attention_mask': attention_mask, 'labels': labels}\n",
    "\n",
    "# Apply tokenization function with batching\n",
    "train_data = train_data.map(extract_and_tokenize_function, batched=True, remove_columns=train_data.column_names)\n",
    "val_data = val_data.map(extract_and_tokenize_function, batched=True, remove_columns=val_data.column_names)\n",
    "\n",
    "train_data.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "val_data.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/haiku-gpt2/lib/python3.9/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",  # Save strategy set to \"epoch\"\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/123 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|â–Š         | 10/123 [03:42<47:21, 25.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.8446, 'grad_norm': 17.9531307220459, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|â–ˆâ–‹        | 20/123 [08:01<43:10, 25.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.6471, 'grad_norm': 15.744894981384277, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|â–ˆâ–ˆâ–       | 30/123 [12:08<37:26, 24.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.6936, 'grad_norm': 13.247477531433105, 'learning_rate': 3e-06, 'epoch': 0.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 40/123 [14:49<17:39, 12.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.1773, 'grad_norm': 15.666923522949219, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 41/123 [15:13<20:09, 14.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.7638936042785645, 'eval_runtime': 5.3768, 'eval_samples_per_second': 1.86, 'eval_steps_per_second': 0.93, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 50/123 [18:09<17:57, 14.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.563, 'grad_norm': 13.233949661254883, 'learning_rate': 5e-06, 'epoch': 1.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 60/123 [20:56<16:06, 15.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.4297, 'grad_norm': 12.892355918884277, 'learning_rate': 6e-06, 'epoch': 1.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 70/123 [22:56<08:17,  9.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.07, 'grad_norm': 13.70809555053711, 'learning_rate': 7.000000000000001e-06, 'epoch': 1.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 80/123 [23:44<02:34,  3.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.9739, 'grad_norm': 14.871001243591309, 'learning_rate': 8.000000000000001e-06, 'epoch': 1.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 82/123 [23:58<03:04,  4.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.0768918991088867, 'eval_runtime': 3.1898, 'eval_samples_per_second': 3.135, 'eval_steps_per_second': 1.568, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 90/123 [25:11<02:43,  4.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6421, 'grad_norm': 14.238887786865234, 'learning_rate': 9e-06, 'epoch': 2.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 100/123 [25:49<01:42,  4.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2724, 'grad_norm': 16.64166831970215, 'learning_rate': 1e-05, 'epoch': 2.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 110/123 [26:52<01:45,  8.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5263, 'grad_norm': 19.697540283203125, 'learning_rate': 1.1000000000000001e-05, 'epoch': 2.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 120/123 [29:40<00:51, 17.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3625, 'grad_norm': 13.184134483337402, 'learning_rate': 1.2e-05, 'epoch': 2.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 123/123 [30:28<00:00, 15.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.963078737258911, 'eval_runtime': 2.17, 'eval_samples_per_second': 4.608, 'eval_steps_per_second': 2.304, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 123/123 [31:19<00:00, 15.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1879.9743, 'train_samples_per_second': 0.131, 'train_steps_per_second': 0.065, 'train_loss': 3.4029242856715753, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=123, training_loss=3.4029242856715753, metrics={'train_runtime': 1879.9743, 'train_samples_per_second': 0.131, 'train_steps_per_second': 0.065, 'total_flos': 52279211520000.0, 'train_loss': 3.4029242856715753, 'epoch': 3.0})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fine-tune the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./fine-tuned-haiku-model/tokenizer_config.json',\n",
       " './fine-tuned-haiku-model/special_tokens_map.json',\n",
       " './fine-tuned-haiku-model/vocab.json',\n",
       " './fine-tuned-haiku-model/merges.txt',\n",
       " './fine-tuned-haiku-model/added_tokens.json')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the fine-tuned model\n",
    "model.save_pretrained(\"./fine-tuned-haiku-model\")\n",
    "tokenizer.save_pretrained(\"./fine-tuned-haiku-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate haiku\n",
    "def generate_haiku(prompt, model, tokenizer, max_length=30):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    input_ids = inputs['input_ids']\n",
    "    attention_mask = inputs['attention_mask']\n",
    "    outputs = model.generate(\n",
    "        input_ids=input_ids, \n",
    "        attention_mask=attention_mask, \n",
    "        max_length=max_length, \n",
    "        num_return_sequences=1, \n",
    "        no_repeat_ngram_size=2, \n",
    "        early_stopping= True\n",
    "    )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/haiku-gpt2/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:563: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Haiku:\n",
      "The oceans breeze,\n",
      "Nature's symphony,\n",
      "\n",
      "\n",
      "Life's gentle symphonic dance.\n",
      "\n",
      "...\n",
      "\n",
      ".\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the fine-tuned model\n",
    "prompt = \"The oceans breeze\"\n",
    "haiku = generate_haiku(prompt, model, tokenizer)\n",
    "print(f\"Generated Haiku:\\n{haiku}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fineTune",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
