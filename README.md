# GPT-2 Haiku Generator

This project involves fine-tuning a GPT-2 model to generate haikus. While the model's performance is not perfect, it demonstrates a basic capability to generate information out of the GPT2 model. 

## Overview

The GPT-2 Haiku Generator is a fun and experimental project aimed at exploring the potential of fine-tuning a pre-trained language model to create poetry.

## Fine-Tuning Process

- **Model**: GPT-2
- **Dataset**: "davanstrien/haiku_kto" from Hugging Face, containing approximately 90 samples.
- **Quantization**: 4-bit quantization was employed for efficient model loading.
- **Training and Validation Split**: The dataset was split into training and validation sets.


